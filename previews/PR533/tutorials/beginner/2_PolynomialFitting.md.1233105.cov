        2 ```@meta
     1131 EditURL = "../../../../examples/PolynomialFitting/main.jl"
      505 ```
      751 
      258 # Fitting a Polynomial using MLP
      252 
      250 In this tutorial we will fit a MultiLayer Perceptron (MLP) on data generated from a
      500 polynomial.
        3 
        - ## Package Imports
        4 
        - ````julia
        1 using Lux, LuxAMDGPU, LuxCUDA, Optimisers, Random, Statistics, Zygote
        - using CairoMakie, MakiePublication
        - ````
        - 
        - ## Dataset
        - 
        - Generate 128 datapoints from the polynomial $y = x^2 - 2x$.
        - 
        - ````julia
        - function generate_data(rng::AbstractRNG)
        -     x = reshape(collect(range(-2.0f0, 2.0f0, 128)), (1, 128))
        -     y = evalpoly.(x, ((0, -2, 1),)) .+ randn(rng, (1, 128)) .* 0.1f0
        -     return (x, y)
        - end
        - ````
        - 
        - ````
        - generate_data (generic function with 1 method)
        - ````
        - 
        - Initialize the random number generator and fetch the dataset.
        - 
        - ````julia
        - rng = MersenneTwister()
        - Random.seed!(rng, 12345)
        - 
        - (x, y) = generate_data(rng)
        - ````
        - 
        - ````
        - (Float32[-2.0 -1.968504 -1.9370079 -1.9055119 -1.8740157 -1.8425196 -1.8110236 -1.7795275 -1.7480315 -1.7165354 -1.6850394 -1.6535434 -1.6220472 -1.5905511 -1.5590551 -1.527559 -1.496063 -1.464567 -1.4330709 -1.4015749 -1.3700787 -1.3385826 -1.3070866 -1.2755905 -1.2440945 -1.2125984 -1.1811024 -1.1496063 -1.1181102 -1.0866141 -1.0551181 -1.023622 -0.992126 -0.96062994 -0.92913383 -0.8976378 -0.86614174 -0.8346457 -0.8031496 -0.77165353 -0.7401575 -0.70866144 -0.6771653 -0.6456693 -0.61417323 -0.5826772 -0.5511811 -0.51968503 -0.48818898 -0.4566929 -0.42519686 -0.39370078 -0.36220473 -0.33070865 -0.2992126 -0.26771653 -0.23622048 -0.20472442 -0.17322835 -0.14173229 -0.11023622 -0.07874016 -0.047244094 -0.015748031 0.015748031 0.047244094 0.07874016 0.11023622 0.14173229 0.17322835 0.20472442 0.23622048 0.26771653 0.2992126 0.33070865 0.36220473 0.39370078 0.42519686 0.4566929 0.48818898 0.51968503 0.5511811 0.5826772 0.61417323 0.6456693 0.6771653 0.70866144 0.7401575 0.77165353 0.8031496 0.8346457 0.8661417
        - ````
        - 
        - Let's visualize the dataset
        - 
        - ````julia
        - with_theme(theme_web()) do
        -     fig = Figure()
        -     ax = CairoMakie.Axis(fig[1, 1]; xlabel="x", ylabel="y")
        - 
        -     l = lines!(ax, x[1, :], x -> evalpoly(x, (0, -2, 1)); linewidth=3)
        -     s = scatter!(ax, x[1, :], y[1, :]; markersize=8, color=:orange,
        -         strokecolor=:black, strokewidth=1)
        - 
        -     axislegend(ax, [l, s], ["True Quadratic Function", "Data Points"])
        - 
        -     return fig
        - end
        - ````
        - 
        - ```@raw html
        - <img width=600 height=408 style='object-fit: contain; height: auto;' src="data:image/png;base64, iVBORw0KGgoAAAANSUhEUgAABLAAAAMwCAIAAAC/VA7OAAAABmJLR0QA/wD/AP+gvaeTAAAgAElEQVR4nOzdd1xTV/8H8G8SCBtFUAQEEZAtigvBBY5aV6sVq1XrbKtSR0WttVQ7tI4661ZaR/s4+rhbcdUNgoLIEpGlgIAgQ2aYSZ4/bp80hhVWEpLP+9U/wrkn535z5fn98uHcew5LKBQSAAAAAAAAqB62vAsAAAAAAAAA+UAgBAAAAAAAUFEIhAAAAAAAACoKgRAAAAAAAEBFIRACAAAAAACoKARCAAAAAAAAFYVACAAAAAAAoKIQCAEAAAAAAFQUAiEAAAAAAICKQiAEAAAAAABQUQiEAAAAAAAAKgqBEAAAAAAAQEUhEAIAAAAAAKgoBEIAAAAAAAAVhUAIAAAAAACgohAIAQAAAAAAVBQCIQAAAAAAgIpCIAQAAAAAAFBRCIQAAAAAAAAqCoEQAAAAAABARSEQAgAAAAAAqCgEQgAAAAAAABWFQAgAAAAAAKCiEAgBAAAAAABUFAIhAAAAAACAikIgBAAAAAAAUFEIhAAAAAAAACoKgRAAAAAAAEBFIRACAAAAAACoKARCAAAAAAAAFaUm7wLaABaLJe8SAAAAAABARQmFwtYbHDOEAAAAAAAAKgozhNJq1VwOAAAAAAAgQQb3KmKGEAAAAAAAQEUhEAIAAAAAAKgoBEIAAAAAAAAVhUAIAAAAAACgohAIAQAAAAAAVBQCIQAAAAAAgIpCIAQAAAAAAFBRCIQAAAAAAAAqCoEQAAAAAABARanJu4A2w8zMTPQ6IyNDjpUAAAAAAAC0CJZQKJR3DYqOxWJJtOCiAQAAAABAa2OSSKumD8wQSis9PV3eJQAAAAAAALQkzBA2TAa5HAAAAAAAQIIMkggWlQEAAAAAAFBRCI
        - ```
        - 
        - ## Neural Network
        - 
        - For this problem, you should not be using a neural network. But let's still do that!
        - 
        - ````julia
        - model = Chain(Dense(1 => 16, relu), Dense(16 => 1))
        - ````
        - 
        - ````
        - Chain(
        -     layer_1 = Dense(1 => 16, relu),     # 32 parameters
        -     layer_2 = Dense(16 => 1),           # 17 parameters
        - )         # Total: 49 parameters,
        -           #        plus 0 states.
        - ````
        - 
        - ## Optimizer
        - 
        - We will use Adam from Optimisers.jl
        - 
        - ````julia
        - opt = Adam(0.03f0)
        - ````
        - 
        - ````
        - Adam(0.03, (0.9, 0.999), 1.0e-8)
        - ````
        - 
        - ## Loss Function
        - 
        - We will use the `Lux.Training` API so we need to ensure that our loss function takes 4
        - inputs -- model, parameters, states and data. The function must return 3 values -- loss,
        - updated_state, and any computed statistics.
        - 
        - ````julia
        - function loss_function(model, ps, st, data)
        -     y_pred, st = Lux.apply(model, data[1], ps, st)
        -     mse_loss = mean(abs2, y_pred .- data[2])
        -     return mse_loss, st, ()
        - end
        - ````
        - 
        - ````
        - loss_function (generic function with 1 method)
        - ````
        - 
        - ## Training
        - 
        - First we will create a [`Lux.Experimental.TrainState`](@ref) which is essentially a
        - convenience wrapper over parameters, states and optimizer states.
        - 
        - ````julia
        - tstate = Lux.Training.TrainState(rng, model, opt)
        - ````
        - 
        - ````
        - Lux.Experimental.TrainState{Lux.Chain{@NamedTuple{layer_1::Lux.Dense{true, typeof(NNlib.relu), typeof(WeightInitializers.glorot_uniform), typeof(WeightInitializers.zeros32)}, layer_2::Lux.Dense{true, typeof(identity), typeof(WeightInitializers.glorot_uniform), typeof(WeightInitializers.zeros32)}}, Nothing}, @NamedTuple{layer_1::@NamedTuple{weight::CUDA.CuArray{Float32, 2, CUDA.Mem.DeviceBuffer}, bias::CUDA.CuArray{Float32, 2, CUDA.Mem.DeviceBuffer}}, layer_2::@NamedTuple{weight::CUDA.CuArray{Float32, 2, CUDA.Mem.DeviceBuffer}, bias::CUDA.CuArray{Float32, 2, CUDA.Mem.DeviceBuffer}}}, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}}, @NamedTuple{layer_1::@NamedTuple{weight::Optimisers.Leaf{Optimisers.Adam, Tuple{CUDA.CuArray{Float32, 2, CUDA.Mem.DeviceBuffer}, CUDA.CuArray{Float32, 2, CUDA.Mem.DeviceBuffer}, Tuple{Float32, Float32}}}, bias::Optimisers.Leaf{Optimisers.Adam, Tuple{CUDA.CuArray{Float32, 2, CUDA.Mem.DeviceBuffer}, CUDA.CuArray{Float32, 2, CUDA.Mem.DeviceBuffer}, Tuple{Float32, Float32}}
        - ````
        - 
        - Now we will use Zygote for our AD requirements.
        - 
        - ````julia
        - vjp_rule = Lux.Training.AutoZygote()
        - ````
        - 
        - ````
        - ADTypes.AutoZygote()
        - ````
        - 
        - Finally the training loop.
        - 
        - ````julia
        - function main(tstate::Lux.Experimental.TrainState, vjp, data, epochs)
        -     data = data .|> gpu_device()
        -     for epoch in 1:epochs
        -         grads, loss, stats, tstate = Lux.Training.compute_gradients(
        -             vjp, loss_function, data, tstate)
        -         println("Epoch: $(epoch) || Loss: $(loss)")
        -         tstate = Lux.Training.apply_gradients(tstate, grads)
        -     end
        -     return tstate
        - end
        - 
        - dev_cpu = cpu_device()
        - dev_gpu = gpu_device()
        - 
        - tstate = main(tstate, vjp_rule, (x, y), 250)
        - y_pred = dev_cpu(Lux.apply(tstate.model, dev_gpu(x), tstate.parameters, tstate.states)[1])
        - ````
        - 
        - ````
        - Epoch: 1 || Loss: 9.437286
        - Epoch: 2 || Loss: 9.437286
        - Epoch: 3 || Loss: 9.437286
        - Epoch: 4 || Loss: 9.437286
        - Epoch: 5 || Loss: 9.437286
        - Epoch: 6 || Loss: 9.437286
        - Epoch: 7 || Loss: 9.437286
        - Epoch: 8 || Loss: 9.437286
        - Epoch: 9 || Loss: 9.437286
        - Epoch: 10 || Loss: 9.437286
        - Epoch: 11 || Loss: 9.437286
        - Epoch: 12 || Loss: 9.437286
        - Epoch: 13 || Loss: 9.437286
        - Epoch: 14 || Loss: 9.437286
        - Epoch: 15 || Loss: 9.437286
        - Epoch: 16 || Loss: 9.437286
        - Epoch: 17 || Loss: 9.437286
        - Epoch: 18 || Loss: 9.437286
        - Epoch: 19 || Loss: 9.437286
        - Epoch: 20 || Loss: 9.437286
        - Epoch: 21 || Loss: 9.437286
        - Epoch: 22 || Loss: 9.437286
        - Epoch: 23 || Loss: 9.437286
        - Epoch: 24 || Loss: 9.437286
        - Epoch: 25 || Loss: 9.437286
        - Epoch: 26 || Loss: 9.437286
        - Epoch: 27 || Loss: 9.437286
        - Epoch: 28 || Loss: 9.437286
        - Epoch: 29 || Loss: 9.437286
        - Epoch: 30 || Loss: 9.437286
        - Epoch: 31 || Loss: 9.437286
        - Epoch: 32 || Loss: 9.437286
        - Epoch: 33 || Loss: 9.437286
        - Epoch: 34 || Loss: 9.437286
        - Epoch: 35 || Loss: 9.437286
        - Epoch: 36 || Loss: 9.437286
        - Epoch: 37 || Loss: 9.437286
        - Epoch: 38 || Loss: 9.437286
        - Epoch: 39 || Loss: 9.437286
        - Epoch: 40 || Loss: 9.437286
        - Epoch: 41 || Loss: 9.437286
        - Epoch: 42 || Loss: 9.437286
        - Epoch: 43 || Loss: 9.437286
        - Epoch: 44 || Loss: 9.437286
        - Epoch: 45 || Loss: 9.437286
        - Epoch: 46 || Loss: 9.437286
        - Epoch: 47 || Loss: 9.437286
        - Epoch: 48 || Loss: 9.437286
        - Epoch: 49 || Loss: 9.437286
        - Epoch: 50 || Loss: 9.437286
        - Epoch: 51 || Loss: 9.437286
        - Epoch: 52 || Loss: 9.437286
        - Epoch: 53 || Loss: 9.437286
        - Epoch: 54 || Loss: 9.437286
        - Epoch: 55 || Loss: 9.437286
        - Epoch: 56 || Loss: 9.437286
        - Epoch: 57 || Loss: 9.437286
        - Epoch: 58 || Loss: 9.437286
        - Epoch: 59 || Loss: 9.437286
        - Epoch: 60 || Loss: 9.437286
        - Epoch: 61 || Loss: 9.437286
        - Epoch: 62 || Loss: 9.437286
        - Epoch: 63 || Loss: 9.437286
        - Epoch: 64 || Loss: 9.437286
        - Epoch: 65 || Loss: 9.437286
        - Epoch: 66 || Loss: 9.437286
        - Epoch: 67 || Loss: 9.437286
        - Epoch: 68 || Loss: 9.437286
        - Epoch: 69 || Loss: 9.437286
        - Epoch: 70 || Loss: 9.437286
        - Epoch: 71 || Loss: 9.437286
        - Epoch: 72 || Loss: 9.437286
        - Epoch: 73 || Loss: 9.437286
        - Epoch: 74 || Loss: 9.437286
        - Epoch: 75 || Loss: 9.437286
        - Epoch: 76 || Loss: 9.437286
        - Epoch: 77 || Loss: 9.437286
        - Epoch: 78 || Loss: 9.437286
        - Epoch: 79 || Loss: 9.437286
        - Epoch: 80 || Loss: 9.437286
        - Epoch: 81 || Loss: 9.437286
        - Epoch: 82 || Loss: 9.437286
        - Epoch: 83 || Loss: 9.437286
        - Epoch: 84 || Loss: 9.437286
        - Epoch: 85 || Loss: 9.437286
        - Epoch: 86 || Loss: 9.437286
        - Epoch: 87 || Loss: 9.437286
        - Epoch: 88 || Loss: 9.437286
        - Epoch: 89 || Loss: 9.437286
        - Epoch: 90 || Loss: 9.437286
        - Epoch: 91 || Loss: 9.437286
        - Epoch: 92 || Loss: 9.437286
        - Epoch: 93 || Loss: 9.437286
        - Epoch: 94 || Loss: 9.437286
        - Epoch: 95 || Loss: 9.437286
        - Epoch: 96 || Loss: 9.437286
        - Epoch: 97 || Loss: 9.437286
        - Epoch: 98 || Loss: 9.437286
        - Epoch: 99 || Loss: 9.437286
        - Epoch: 100 || Loss: 9.437286
        - Epoch: 101 || Loss: 9.437286
        - Epoch: 102 || Loss: 9.437286
        - Epoch: 103 || Loss: 9.437286
        - Epoch: 104 || Loss: 9.437286
        - Epoch: 105 || Loss: 9.437286
        - Epoch: 106 || Loss: 9.437286
        - Epoch: 107 || Loss: 9.437286
        - Epoch: 108 || Loss: 9.437286
        - Epoch: 109 || Loss: 9.437286
        - Epoch: 110 || Loss: 9.437286
        - Epoch: 111 || Loss: 9.437286
        - Epoch: 112 || Loss: 9.437286
        - Epoch: 113 || Loss: 9.437286
        - Epoch: 114 || Loss: 9.437286
        - Epoch: 115 || Loss: 9.437286
        - Epoch: 116 || Loss: 9.437286
        - Epoch: 117 || Loss: 9.437286
        - Epoch: 118 || Loss: 9.437286
        - Epoch: 119 || Loss: 9.437286
        - Epoch: 120 || Loss: 9.437286
        - Epoch: 121 || Loss: 9.437286
        - Epoch: 122 || Loss: 9.437286
        - Epoch: 123 || Loss: 9.437286
        - Epoch: 124 || Loss: 9.437286
        - Epoch: 125 || Loss: 9.437286
        - Epoch: 126 || Loss: 9.437286
        - Epoch: 127 || Loss: 9.437286
        - Epoch: 128 || Loss: 9.437286
        - Epoch: 129 || Loss: 9.437286
        - Epoch: 130 || Loss: 9.437286
        - Epoch: 131 || Loss: 9.437286
        - Epoch: 132 || Loss: 9.437286
        - Epoch: 133 || Loss: 9.437286
        - Epoch: 134 || Loss: 9.437286
        - Epoch: 135 || Loss: 9.437286
        - Epoch: 136 || Loss: 9.437286
        - Epoch: 137 || Loss: 9.437286
        - Epoch: 138 || Loss: 9.437286
        - Epoch: 139 || Loss: 9.437286
        - Epoch: 140 || Loss: 9.437286
        - Epoch: 141 || Loss: 9.437286
        - Epoch: 142 || Loss: 9.437286
        - Epoch: 143 || Loss: 9.437286
        - Epoch: 144 || Loss: 9.437286
        - Epoch: 145 || Loss: 9.437286
        - Epoch: 146 || Loss: 9.437286
        - Epoch: 147 || Loss: 9.437286
        - Epoch: 148 || Loss: 9.437286
        - Epoch: 149 || Loss: 9.437286
        - Epoch: 150 || Loss: 9.437286
        - Epoch: 151 || Loss: 9.437286
        - Epoch: 152 || Loss: 9.437286
        - Epoch: 153 || Loss: 9.437286
        - Epoch: 154 || Loss: 9.437286
        - Epoch: 155 || Loss: 9.437286
        - Epoch: 156 || Loss: 9.437286
        - Epoch: 157 || Loss: 9.437286
        - Epoch: 158 || Loss: 9.437286
        - Epoch: 159 || Loss: 9.437286
        - Epoch: 160 || Loss: 9.437286
        - Epoch: 161 || Loss: 9.437286
        - Epoch: 162 || Loss: 9.437286
        - Epoch: 163 || Loss: 9.437286
        - Epoch: 164 || Loss: 9.437286
        - Epoch: 165 || Loss: 9.437286
        - Epoch: 166 || Loss: 9.437286
        - Epoch: 167 || Loss: 9.437286
        - Epoch: 168 || Loss: 9.437286
        - Epoch: 169 || Loss: 9.437286
        - Epoch: 170 || Loss: 9.437286
        - Epoch: 171 || Loss: 9.437286
        - Epoch: 172 || Loss: 9.437286
        - Epoch: 173 || Loss: 9.437286
        - Epoch: 174 || Loss: 9.437286
        - Epoch: 175 || Loss: 9.437286
        - Epoch: 176 || Loss: 9.437286
        - Epoch: 177 || Loss: 9.437286
        - Epoch: 178 || Loss: 9.437286
        - Epoch: 179 || Loss: 9.437286
        - Epoch: 180 || Loss: 9.437286
        - Epoch: 181 || Loss: 9.437286
        - Epoch: 182 || Loss: 9.437286
        - Epoch: 183 || Loss: 9.437286
        - Epoch: 184 || Loss: 9.437286
        - Epoch: 185 || Loss: 9.437286
        - Epoch: 186 || Loss: 9.437286
        - Epoch: 187 || Loss: 9.437286
        - Epoch: 188 || Loss: 9.437286
        - Epoch: 189 || Loss: 9.437286
        - Epoch: 190 || Loss: 9.437286
        - Epoch: 191 || Loss: 9.437286
        - Epoch: 192 || Loss: 9.437286
        - Epoch: 193 || Loss: 9.437286
        - Epoch: 194 || Loss: 9.437286
        - Epoch: 195 || Loss: 9.437286
        - Epoch: 196 || Loss: 9.437286
        - Epoch: 197 || Loss: 9.437286
        - Epoch: 198 || Loss: 9.437286
        - Epoch: 199 || Loss: 9.437286
        - Epoch: 200 || Loss: 9.437286
        - Epoch: 201 || Loss: 9.437286
        - Epoch: 202 || Loss: 9.437286
        - Epoch: 203 || Loss: 9.437286
        - Epoch: 204 || Loss: 9.437286
        - Epoch: 205 || Loss: 9.437286
        - Epoch: 206 || Loss: 9.437286
        - Epoch: 207 || Loss: 9.437286
        - Epoch: 208 || Loss: 9.437286
        - Epoch: 209 || Loss: 9.437286
        - Epoch: 210 || Loss: 9.437286
        - Epoch: 211 || Loss: 9.437286
        - Epoch: 212 || Loss: 9.437286
        - Epoch: 213 || Loss: 9.437286
        - Epoch: 214 || Loss: 9.437286
        - Epoch: 215 || Loss: 9.437286
        - Epoch: 216 || Loss: 9.437286
        - Epoch: 217 || Loss: 9.437286
        - Epoch: 218 || Loss: 9.437286
        - Epoch: 219 || Loss: 9.437286
        - Epoch: 220 || Loss: 9.437286
        - Epoch: 221 || Loss: 9.437286
        - Epoch: 222 || Loss: 9.437286
        - Epoch: 223 || Loss: 9.437286
        - Epoch: 224 || Loss: 9.437286
        - Epoch: 225 || Loss: 9.437286
        - Epoch: 226 || Loss: 9.437286
        - Epoch: 227 || Loss: 9.437286
        - Epoch: 228 || Loss: 9.437286
        - Epoch: 229 || Loss: 9.437286
        - Epoch: 230 || Loss: 9.437286
        - Epoch: 231 || Loss: 9.437286
        - Epoch: 232 || Loss: 9.437286
        - Epoch: 233 || Loss: 9.437286
        - Epoch: 234 || Loss: 9.437286
        - Epoch: 235 || Loss: 9.437286
        - Epoch: 236 || Loss: 9.437286
        - Epoch: 237 || Loss: 9.437286
        - Epoch: 238 || Loss: 9.437286
        - Epoch: 239 || Loss: 9.437286
        - Epoch: 240 || Loss: 9.437286
        - Epoch: 241 || Loss: 9.437286
        - Epoch: 242 || Loss: 9.437286
        - Epoch: 243 || Loss: 9.437286
        - Epoch: 244 || Loss: 9.437286
        - Epoch: 245 || Loss: 9.437286
        - Epoch: 246 || Loss: 9.437286
        - Epoch: 247 || Loss: 9.437286
        - Epoch: 248 || Loss: 9.437286
        - Epoch: 249 || Loss: 9.437286
        - Epoch: 250 || Loss: 9.437286
        - 
        - ````
        - 
        - Let's plot the results
        - 
        - ````julia
        - with_theme(theme_web()) do
        -     fig = Figure()
        -     ax = CairoMakie.Axis(fig[1, 1]; xlabel="x", ylabel="y")
        - 
        -     l = lines!(ax, x[1, :], x -> evalpoly(x, (0, -2, 1)); linewidth=3)
        -     s1 = scatter!(ax, x[1, :], y[1, :]; markersize=8, color=:orange,
        -         strokecolor=:black, strokewidth=1)
        -     s2 = scatter!(ax, x[1, :], y_pred[1, :]; markersize=8,
        -         color=:green, strokecolor=:black, strokewidth=1)
        - 
        -     axislegend(ax, [l, s1, s2], ["True Quadratic Function", "Actual Data", "Predictions"])
        - 
        -     return fig
        - end
        - ````
        - 
        - ```@raw html
        - <img width=600 height=408 style='object-fit: contain; height: auto;' src="data:image/png;base64, iVBORw0KGgoAAAANSUhEUgAABLAAAAMwCAIAAAC/VA7OAAAABmJLR0QA/wD/AP+gvaeTAAAgAElEQVR4nOzdaVyUVd8H8N8wgOyKoICgIotsbrghKCphWS5ZiWvmWrlkmqilkuadpmiK5p6WZt1uj5pYmkvuoCgugKgILiCCgiyiwLAzz4urexqHbUCYYfl9P7wYznXmnP91xA/z55zrHJFUKgURERERERE1PBrqDoCIiIiIiIjUgwkhERERERFRA8WEkIiIiIiIqIFiQkhERERERNRAMSEkIiIiIiJqoJgQEhERERERNVBMCImIiIiIiBooJoREREREREQNFBNCIiIiIiKiBooJIRERERERUQPFhJCIiIiIiKiBYkJIRERERETUQDEhJCIiIiIiaqCYEBIRERERETVQTAiJiIiIiIgaKCaEREREREREDRQTQiIiIiIiogaKCSEREREREVEDxYSQiIiIiIiogWJCSERERERE1EAxISQiIiIiImqgmBASERERERE1UEwIiYiIiIiIGigmhERERERERA0UE0IiIiIiIqIGigkhERERERFRA8WEkIiIiIiIqIFiQkhERERERNRAMSEkIiIiIiJqoJgQEhERERERNVCa6g6gDhCJROoOgYiIiIiIGiipVFpzjXOGkIiIiIiIqIHiDKGyajQvJyIiIiIiUqCCtYqcISQiIiIiImqgmBASERERERE1UEwIiYiIiIiIGigmhERERERERA0UE0IiIiIiIqIGigkhERERERFRA8WEkIiIiIiIqIFiQkhERERERNRAMSEkIiIiIiJqoDTVHUCdYWlpKXudmJioxkiIiIiIiIiqhUgqlao7htpOJBIplHDQiIiIiIiopgmZSI1mH5whVFZCQoK6QyAiIiIiIqpOnCGsmAryciIiIiIiIgUqyE
        - ```
        - 
        - ## Appendix
        - 
        - ````julia
        - using InteractiveUtils
        - InteractiveUtils.versioninfo()
        - if @isdefined(LuxCUDA) && CUDA.functional(); println(); CUDA.versioninfo(); end
        - if @isdefined(LuxAMDGPU) && LuxAMDGPU.functional(); println(); AMDGPU.versioninfo(); end
        - ````
        - 
        - ````
        - Julia Version 1.10.2
        - Commit bd47eca2c8a (2024-03-01 10:14 UTC)
        - Build Info:
        -   Official https://julialang.org/ release
        - Platform Info:
        -   OS: Linux (x86_64-linux-gnu)
        -   CPU: 48 × AMD EPYC 7402 24-Core Processor
        -   WORD_SIZE: 64
        -   LIBM: libopenlibm
        -   LLVM: libLLVM-15.0.7 (ORCJIT, znver2)
        - Threads: 48 default, 0 interactive, 24 GC (on 2 virtual cores)
        - Environment:
        -   LD_LIBRARY_PATH = /usr/local/nvidia/lib:/usr/local/nvidia/lib64
        -   JULIA_DEPOT_PATH = /root/.cache/julia-buildkite-plugin/depots/01872db4-8c79-43af-ab7d-12abac4f24f6
        -   JULIA_PROJECT = /var/lib/buildkite-agent/builds/gpuci-10/julialang/lux-dot-jl/docs/Project.toml
        -   JULIA_AMDGPU_LOGGING_ENABLED = true
        -   JULIA_DEBUG = Literate
        -   JULIA_CPU_THREADS = 2
        -   JULIA_NUM_THREADS = 48
        -   JULIA_LOAD_PATH = @:@v#.#:@stdlib
        -   JULIA_CUDA_HARD_MEMORY_LIMIT = 25%
        - 
        - CUDA runtime 12.3, artifact installation
        - CUDA driver 12.3
        - NVIDIA driver 545.23.8
        - 
        - CUDA libraries: 
        - - CUBLAS: 12.3.4
        - - CURAND: 10.3.4
        - - CUFFT: 11.0.12
        - - CUSOLVER: 11.5.4
        - - CUSPARSE: 12.2.0
        - - CUPTI: 21.0.0
        - - NVML: 12.0.0+545.23.8
        - 
        - Julia packages: 
        - - CUDA: 5.2.0
        - - CUDA_Driver_jll: 0.7.0+1
        - - CUDA_Runtime_jll: 0.11.1+0
        - 
        - Toolchain:
        - - Julia: 1.10.2
        - - LLVM: 15.0.7
        - 
        - Environment:
        - - JULIA_CUDA_HARD_MEMORY_LIMIT: 25%
        - 
        - 1 device:
        -   0: NVIDIA A100-PCIE-40GB MIG 1g.5gb (sm_80, 4.130 GiB / 4.750 GiB available)
        - ┌ Warning: LuxAMDGPU is loaded but the AMDGPU is not functional.
        - └ @ LuxAMDGPU ~/.cache/julia-buildkite-plugin/depots/01872db4-8c79-43af-ab7d-12abac4f24f6/packages/LuxAMDGPU/sGa0S/src/LuxAMDGPU.jl:19
        - 
        - ````
        - 
        - ---
        - 
        - *This page was generated using [Literate.jl](https://github.com/fredrikekre/Literate.jl).*
        - 
