       55 ```@meta
        2 EditURL = "../../../../examples/SimpleRNN/main.jl"
        4 ```
        - 
        1 # Training a Simple LSTM
       52 
      451 In this tutorial we will go over using a recurrent neural network to classify clockwise
      852 and anticlockwise spirals. By the end of this tutorial you will be able to:
      401 
        - 1. Create custom Lux models.
       52 2. Become familiar with the Lux recurrent neural network API.
     9901 3. Training using Optimisers.jl and Zygote.jl.
    20002 
    28175 ## Package Imports
        1 
      501 ````julia
       50 using Lux, LuxAMDGPU, LuxCUDA, JLD2, MLUtils, Optimisers, Zygote, Random, Statistics
      401 ````
      575 
       25 ## Dataset
      175 
      175 We will use MLUtils to generate 500 (noisy) clockwise and 500 (noisy) anticlockwise
      175 spirals. Using this data we will create a `MLUtils.DataLoader`. Our dataloader will give
      175 us sequences of size 2 × seq_len × batch_size and we need to predict a binary value
      175 whether the sequence is clockwise or anticlockwise.
        - 
      175 ````julia
      175 function get_dataloaders(; dataset_size=1000, sequence_length=50)
        -     # Create the spirals
        -     data = [MLUtils.Datasets.make_spiral(sequence_length) for _ in 1:dataset_size]
       25     # Get the labels
       25     labels = vcat(repeat([0.0f0], dataset_size ÷ 2), repeat([1.0f0], dataset_size ÷ 2))
       50     clockwise_spirals = [reshape(d[1][:, 1:sequence_length], :, sequence_length, 1)
       50                          for d in data[1:(dataset_size ÷ 2)]]
       50     anticlockwise_spirals = [reshape(
       50                                  d[1][:, (sequence_length + 1):end], :, sequence_length, 1)
      100                              for d in data[((dataset_size ÷ 2) + 1):end]]
       50     x_data = Float32.(cat(clockwise_spirals..., anticlockwise_spirals...; dims=3))
       49     # Split the dataset
        -     (x_train, y_train), (x_val, y_val) = splitobs((x_data, labels); at=0.8, shuffle=true)
        1     # Create DataLoaders
        -     return (
        -         # Use DataLoader to automatically minibatch and shuffle the data
        -         DataLoader(collect.((x_train, y_train)); batchsize=128, shuffle=true),
        -         # Don't shuffle the validation data
        -         DataLoader(collect.((x_val, y_val)); batchsize=128, shuffle=false))
        - end
        - ````
        - 
        - ````
        - get_dataloaders (generic function with 1 method)
        - ````
        - 
        - ## Creating a Classifier
        - 
        - We will be extending the `Lux.AbstractExplicitContainerLayer` type for our custom model
        - since it will contain a lstm block and a classifier head.
        - 
        - We pass the fieldnames `lstm_cell` and `classifier` to the type to ensure that the
        - parameters and states are automatically populated and we don't have to define
        - `Lux.initialparameters` and `Lux.initialstates`.
        - 
        - To understand more about container layers, please look at
        - [Container Layer](@ref Container-Layer).
        - 
        - ````julia
        - struct SpiralClassifier{L, C} <:
        -        Lux.AbstractExplicitContainerLayer{(:lstm_cell, :classifier)}
        -     lstm_cell::L
        -     classifier::C
        - end
        - ````
        - 
        - We won't define the model from scratch but rather use the [`Lux.LSTMCell`](@ref) and
        - [`Lux.Dense`](@ref).
        - 
        - ````julia
        - function SpiralClassifier(in_dims, hidden_dims, out_dims)
        -     return SpiralClassifier(
        -         LSTMCell(in_dims => hidden_dims), Dense(hidden_dims => out_dims, sigmoid))
        - end
        - ````
        - 
        - ````
        - Main.var"##225".SpiralClassifier
        - ````
        - 
        - We can use default Lux blocks -- `Recurrence(LSTMCell(in_dims => hidden_dims)` -- instead
        - of defining the following. But let's still do it for the sake of it.
        - 
        - Now we need to define the behavior of the Classifier when it is invoked.
        - 
        - ````julia
        - function (s::SpiralClassifier)(
        -         x::AbstractArray{T, 3}, ps::NamedTuple, st::NamedTuple) where {T}
        -     # First we will have to run the sequence through the LSTM Cell
        -     # The first call to LSTM Cell will create the initial hidden state
        -     # See that the parameters and states are automatically populated into a field called
        -     # `lstm_cell` We use `eachslice` to get the elements in the sequence without copying,
        -     # and `Iterators.peel` to split out the first element for LSTM initialization.
        -     x_init, x_rest = Iterators.peel(Lux._eachslice(x, Val(2)))
        -     (y, carry), st_lstm = s.lstm_cell(x_init, ps.lstm_cell, st.lstm_cell)
        -     # Now that we have the hidden state and memory in `carry` we will pass the input and
        -     # `carry` jointly
        -     for x in x_rest
        -         (y, carry), st_lstm = s.lstm_cell((x, carry), ps.lstm_cell, st_lstm)
        -     end
        -     # After running through the sequence we will pass the output through the classifier
        -     y, st_classifier = s.classifier(y, ps.classifier, st.classifier)
        -     # Finally remember to create the updated state
        -     st = merge(st, (classifier=st_classifier, lstm_cell=st_lstm))
        -     return vec(y), st
        - end
        - ````
        - 
        - ## Defining Accuracy, Loss and Optimiser
        - 
        - Now let's define the binarycrossentropy loss. Typically it is recommended to use
        - `logitbinarycrossentropy` since it is more numerically stable, but for the sake of
        - simplicity we will use `binarycrossentropy`.
        - 
        - ````julia
        - function xlogy(x, y)
        -     result = x * log(y)
        -     return ifelse(iszero(x), zero(result), result)
        - end
        - 
        - function binarycrossentropy(y_pred, y_true)
        -     y_pred = y_pred .+ eps(eltype(y_pred))
        -     return mean(@. -xlogy(y_true, y_pred) - xlogy(1 - y_true, 1 - y_pred))
        - end
        - 
        - function compute_loss(x, y, model, ps, st)
        -     y_pred, st = model(x, ps, st)
        -     return binarycrossentropy(y_pred, y), y_pred, st
        - end
        - 
        - matches(y_pred, y_true) = sum((y_pred .> 0.5f0) .== y_true)
        - accuracy(y_pred, y_true) = matches(y_pred, y_true) / length(y_pred)
        - ````
        - 
        - ````
        - accuracy (generic function with 1 method)
        - ````
        - 
        - Finally lets create an optimiser given the model parameters.
        - 
        - ````julia
        - function create_optimiser(ps)
        -     opt = Optimisers.Adam(0.01f0)
        -     return Optimisers.setup(opt, ps)
        - end
        - ````
        - 
        - ````
        - create_optimiser (generic function with 1 method)
        - ````
        - 
        - ## Training the Model
        - 
        - ````julia
        - function main()
        -     # Get the dataloaders
        -     (train_loader, val_loader) = get_dataloaders()
        - 
        -     # Create the model
        -     model = SpiralClassifier(2, 8, 1)
        -     rng = Random.default_rng()
        -     Random.seed!(rng, 0)
        -     ps, st = Lux.setup(rng, model)
        - 
        -     dev = gpu_device()
        -     ps = ps |> dev
        -     st = st |> dev
        - 
        -     # Create the optimiser
        -     opt_state = create_optimiser(ps)
        - 
        -     for epoch in 1:25
        -         # Train the model
        -         for (x, y) in train_loader
        -             x = x |> dev
        -             y = y |> dev
        -             (loss, y_pred, st), back = pullback(compute_loss, x, y, model, ps, st)
        -             gs = back((one(loss), nothing, nothing))[4]
        -             opt_state, ps = Optimisers.update(opt_state, ps, gs)
        - 
        -             println("Epoch [$epoch]: Loss $loss")
        -         end
        - 
        -         # Validate the model
        -         st_ = Lux.testmode(st)
        -         for (x, y) in val_loader
        -             x = x |> dev
        -             y = y |> dev
        -             (loss, y_pred, st_) = compute_loss(x, y, model, ps, st_)
        -             acc = accuracy(y_pred, y)
        -             println("Validation: Loss $loss Accuracy $acc")
        -         end
        -     end
        - 
        -     return (ps, st) |> cpu_device()
        - end
        - 
        - ps_trained, st_trained = main()
        - ````
        - 
        - ````
        - ┌ Warning: `replicate` doesn't work for `TaskLocalRNG`. Returning the same `TaskLocalRNG`.
        - └ @ LuxCore ~/.cache/julia-buildkite-plugin/depots/01872db4-8c79-43af-ab7d-12abac4f24f6/packages/LuxCore/t4mG0/src/LuxCore.jl:13
        - Epoch [1]: Loss 0.5628675
        - Epoch [1]: Loss 0.5119977
        - Epoch [1]: Loss 0.4728254
        - Epoch [1]: Loss 0.4555329
        - Epoch [1]: Loss 0.41948837
        - Epoch [1]: Loss 0.40569782
        - Epoch [1]: Loss 0.383585
        - Validation: Loss 0.37372476 Accuracy 1.0
        - Validation: Loss 0.37338188 Accuracy 1.0
        - Epoch [2]: Loss 0.37543058
        - Epoch [2]: Loss 0.34196275
        - Epoch [2]: Loss 0.33541268
        - Epoch [2]: Loss 0.31153625
        - Epoch [2]: Loss 0.3031516
        - Epoch [2]: Loss 0.29064986
        - Epoch [2]: Loss 0.26962194
        - Validation: Loss 0.26145047 Accuracy 1.0
        - Validation: Loss 0.26151827 Accuracy 1.0
        - Epoch [3]: Loss 0.25731367
        - Epoch [3]: Loss 0.24292366
        - Epoch [3]: Loss 0.23401418
        - Epoch [3]: Loss 0.22457357
        - Epoch [3]: Loss 0.21166581
        - Epoch [3]: Loss 0.1959871
        - Epoch [3]: Loss 0.18628529
        - Validation: Loss 0.18147554 Accuracy 1.0
        - Validation: Loss 0.181658 Accuracy 1.0
        - Epoch [4]: Loss 0.17902184
        - Epoch [4]: Loss 0.1711165
        - Epoch [4]: Loss 0.16301656
        - Epoch [4]: Loss 0.15399471
        - Epoch [4]: Loss 0.14642717
        - Epoch [4]: Loss 0.14235383
        - Epoch [4]: Loss 0.13550887
        - Validation: Loss 0.12900752 Accuracy 1.0
        - Validation: Loss 0.1291565 Accuracy 1.0
        - Epoch [5]: Loss 0.12687638
        - Epoch [5]: Loss 0.12170105
        - Epoch [5]: Loss 0.116760775
        - Epoch [5]: Loss 0.111698166
        - Epoch [5]: Loss 0.107624255
        - Epoch [5]: Loss 0.10008188
        - Epoch [5]: Loss 0.09774242
        - Validation: Loss 0.093667306 Accuracy 1.0
        - Validation: Loss 0.09377551 Accuracy 1.0
        - Epoch [6]: Loss 0.093345374
        - Epoch [6]: Loss 0.08720921
        - Epoch [6]: Loss 0.087059036
        - Epoch [6]: Loss 0.07970849
        - Epoch [6]: Loss 0.077803254
        - Epoch [6]: Loss 0.07376104
        - Epoch [6]: Loss 0.07203657
        - Validation: Loss 0.06915274 Accuracy 1.0
        - Validation: Loss 0.06922646 Accuracy 1.0
        - Epoch [7]: Loss 0.07000205
        - Epoch [7]: Loss 0.06508204
        - Epoch [7]: Loss 0.0613546
        - Epoch [7]: Loss 0.060703266
        - Epoch [7]: Loss 0.059507474
        - Epoch [7]: Loss 0.052968398
        - Epoch [7]: Loss 0.050128296
        - Validation: Loss 0.051669605 Accuracy 1.0
        - Validation: Loss 0.05171901 Accuracy 1.0
        - Epoch [8]: Loss 0.05037313
        - Epoch [8]: Loss 0.047893904
        - Epoch [8]: Loss 0.046697255
        - Epoch [8]: Loss 0.04469424
        - Epoch [8]: Loss 0.043943267
        - Epoch [8]: Loss 0.04143489
        - Epoch [8]: Loss 0.041350797
        - Validation: Loss 0.039024524 Accuracy 1.0
        - Validation: Loss 0.039043937 Accuracy 1.0
        - Epoch [9]: Loss 0.038401067
        - Epoch [9]: Loss 0.036639035
        - Epoch [9]: Loss 0.03417008
        - Epoch [9]: Loss 0.0333322
        - Epoch [9]: Loss 0.033617884
        - Epoch [9]: Loss 0.03140834
        - Epoch [9]: Loss 0.031438235
        - Validation: Loss 0.029920463 Accuracy 1.0
        - Validation: Loss 0.029922746 Accuracy 1.0
        - Epoch [10]: Loss 0.029573962
        - Epoch [10]: Loss 0.027335634
        - Epoch [10]: Loss 0.027300388
        - Epoch [10]: Loss 0.025966667
        - Epoch [10]: Loss 0.024902333
        - Epoch [10]: Loss 0.024816219
        - Epoch [10]: Loss 0.024321552
        - Validation: Loss 0.023595005 Accuracy 1.0
        - Validation: Loss 0.02357648 Accuracy 1.0
        - Epoch [11]: Loss 0.023073949
        - Epoch [11]: Loss 0.021690156
        - Epoch [11]: Loss 0.02177171
        - Epoch [11]: Loss 0.02013648
        - Epoch [11]: Loss 0.021036316
        - Epoch [11]: Loss 0.020027583
        - Epoch [11]: Loss 0.017281188
        - Validation: Loss 0.019222993 Accuracy 1.0
        - Validation: Loss 0.019203184 Accuracy 1.0
        - Epoch [12]: Loss 0.018354513
        - Epoch [12]: Loss 0.01881507
        - Epoch [12]: Loss 0.018577676
        - Epoch [12]: Loss 0.014990501
        - Epoch [12]: Loss 0.01703481
        - Epoch [12]: Loss 0.016969122
        - Epoch [12]: Loss 0.01607458
        - Validation: Loss 0.01617828 Accuracy 1.0
        - Validation: Loss 0.016155371 Accuracy 1.0
        - Epoch [13]: Loss 0.015956432
        - Epoch [13]: Loss 0.015128081
        - Epoch [13]: Loss 0.01521128
        - Epoch [13]: Loss 0.013981636
        - Epoch [13]: Loss 0.014477943
        - Epoch [13]: Loss 0.0139561575
        - Epoch [13]: Loss 0.014596927
        - Validation: Loss 0.013974616 Accuracy 1.0
        - Validation: Loss 0.013953558 Accuracy 1.0
        - Epoch [14]: Loss 0.013869691
        - Epoch [14]: Loss 0.012413105
        - Epoch [14]: Loss 0.014160939
        - Epoch [14]: Loss 0.012944266
        - Epoch [14]: Loss 0.012019176
        - Epoch [14]: Loss 0.011968395
        - Epoch [14]: Loss 0.012105623
        - Validation: Loss 0.012314909 Accuracy 1.0
        - Validation: Loss 0.012296395 Accuracy 1.0
        - Epoch [15]: Loss 0.011724299
        - Epoch [15]: Loss 0.011771173
        - Epoch [15]: Loss 0.012315014
        - Epoch [15]: Loss 0.011467608
        - Epoch [15]: Loss 0.010834159
        - Epoch [15]: Loss 0.010211762
        - Epoch [15]: Loss 0.0117722275
        - Validation: Loss 0.011020707 Accuracy 1.0
        - Validation: Loss 0.011003961 Accuracy 1.0
        - Epoch [16]: Loss 0.010556351
        - Epoch [16]: Loss 0.010859212
        - Epoch [16]: Loss 0.010274036
        - Epoch [16]: Loss 0.009528779
        - Epoch [16]: Loss 0.010381801
        - Epoch [16]: Loss 0.009826366
        - Epoch [16]: Loss 0.010363013
        - Validation: Loss 0.009973452 Accuracy 1.0
        - Validation: Loss 0.0099579375 Accuracy 1.0
        - Epoch [17]: Loss 0.009545741
        - Epoch [17]: Loss 0.00907085
        - Epoch [17]: Loss 0.009525473
        - Epoch [17]: Loss 0.009454982
        - Epoch [17]: Loss 0.009373775
        - Epoch [17]: Loss 0.008960759
        - Epoch [17]: Loss 0.008696445
        - Validation: Loss 0.009101026 Accuracy 1.0
        - Validation: Loss 0.009087931 Accuracy 1.0
        - Epoch [18]: Loss 0.00886647
        - Epoch [18]: Loss 0.008787328
        - Epoch [18]: Loss 0.009243023
        - Epoch [18]: Loss 0.008214571
        - Epoch [18]: Loss 0.0076314434
        - Epoch [18]: Loss 0.008667259
        - Epoch [18]: Loss 0.0070915767
        - Validation: Loss 0.008365732 Accuracy 1.0
        - Validation: Loss 0.008352558 Accuracy 1.0
        - Epoch [19]: Loss 0.007907672
        - Epoch [19]: Loss 0.00761619
        - Epoch [19]: Loss 0.00840299
        - Epoch [19]: Loss 0.0080338
        - Epoch [19]: Loss 0.007377775
        - Epoch [19]: Loss 0.0076204306
        - Epoch [19]: Loss 0.008007531
        - Validation: Loss 0.007735743 Accuracy 1.0
        - Validation: Loss 0.0077239797 Accuracy 1.0
        - Epoch [20]: Loss 0.007529175
        - Epoch [20]: Loss 0.0068568
        - Epoch [20]: Loss 0.007222345
        - Epoch [20]: Loss 0.0075053805
        - Epoch [20]: Loss 0.0070357267
        - Epoch [20]: Loss 0.00731463
        - Epoch [20]: Loss 0.007454876
        - Validation: Loss 0.0071848948 Accuracy 1.0
        - Validation: Loss 0.007173905 Accuracy 1.0
        - Epoch [21]: Loss 0.0069283014
        - Epoch [21]: Loss 0.0067212963
        - Epoch [21]: Loss 0.0071410555
        - Epoch [21]: Loss 0.006468824
        - Epoch [21]: Loss 0.0066477903
        - Epoch [21]: Loss 0.006722225
        - Epoch [21]: Loss 0.0061233756
        - Validation: Loss 0.006697979 Accuracy 1.0
        - Validation: Loss 0.006687302 Accuracy 1.0
        - Epoch [22]: Loss 0.0065204594
        - Epoch [22]: Loss 0.0064018564
        - Epoch [22]: Loss 0.0063233306
        - Epoch [22]: Loss 0.006257154
        - Epoch [22]: Loss 0.00612626
        - Epoch [22]: Loss 0.0060109203
        - Epoch [22]: Loss 0.006831098
        - Validation: Loss 0.0062671085 Accuracy 1.0
        - Validation: Loss 0.0062571084 Accuracy 1.0
        - Epoch [23]: Loss 0.005899299
        - Epoch [23]: Loss 0.0059287073
        - Epoch [23]: Loss 0.006069382
        - Epoch [23]: Loss 0.0058368184
        - Epoch [23]: Loss 0.005905709
        - Epoch [23]: Loss 0.0057220366
        - Epoch [23]: Loss 0.0059304084
        - Validation: Loss 0.0058799293 Accuracy 1.0
        - Validation: Loss 0.005870852 Accuracy 1.0
        - Epoch [24]: Loss 0.0059624296
        - Epoch [24]: Loss 0.005644123
        - Epoch [24]: Loss 0.005252685
        - Epoch [24]: Loss 0.005674094
        - Epoch [24]: Loss 0.0055454993
        - Epoch [24]: Loss 0.005342222
        - Epoch [24]: Loss 0.0047793943
        - Validation: Loss 0.005532252 Accuracy 1.0
        - Validation: Loss 0.0055238474 Accuracy 1.0
        - Epoch [25]: Loss 0.0053423764
        - Epoch [25]: Loss 0.005325867
        - Epoch [25]: Loss 0.0048880787
        - Epoch [25]: Loss 0.005470786
        - Epoch [25]: Loss 0.0048747817
        - Epoch [25]: Loss 0.0052728066
        - Epoch [25]: Loss 0.0056866915
        - Validation: Loss 0.005222696 Accuracy 1.0
        - Validation: Loss 0.0052142283 Accuracy 1.0
        - 
        - ````
        - 
        - ## Saving the Model
        - 
        - We can save the model using JLD2 (and any other serialization library of your choice)
        - Note that we transfer the model to CPU before saving. Additionally, we recommend that
        - you don't save the model
        - 
        - ````julia
        - @save "trained_model.jld2" {compress = true} ps_trained st_trained
        - ````
        - 
        - Let's try loading the model
        - 
        - ````julia
        - @load "trained_model.jld2" ps_trained st_trained
        - ````
        - 
        - ````
        - 2-element Vector{Symbol}:
        -  :ps_trained
        -  :st_trained
        - ````
        - 
        - ## Appendix
        - 
        - ````julia
        - using InteractiveUtils
        - InteractiveUtils.versioninfo()
        - if @isdefined(LuxCUDA) && CUDA.functional(); println(); CUDA.versioninfo(); end
        - if @isdefined(LuxAMDGPU) && LuxAMDGPU.functional(); println(); AMDGPU.versioninfo(); end
        - ````
        - 
        - ````
        - Julia Version 1.10.2
        - Commit bd47eca2c8a (2024-03-01 10:14 UTC)
        - Build Info:
        -   Official https://julialang.org/ release
        - Platform Info:
        -   OS: Linux (x86_64-linux-gnu)
        -   CPU: 48 × AMD EPYC 7402 24-Core Processor
        -   WORD_SIZE: 64
        -   LIBM: libopenlibm
        -   LLVM: libLLVM-15.0.7 (ORCJIT, znver2)
        - Threads: 48 default, 0 interactive, 24 GC (on 2 virtual cores)
        - Environment:
        -   LD_LIBRARY_PATH = /usr/local/nvidia/lib:/usr/local/nvidia/lib64
        -   JULIA_DEPOT_PATH = /root/.cache/julia-buildkite-plugin/depots/01872db4-8c79-43af-ab7d-12abac4f24f6
        -   JULIA_PROJECT = /var/lib/buildkite-agent/builds/gpuci-6/julialang/lux-dot-jl/docs/Project.toml
        -   JULIA_AMDGPU_LOGGING_ENABLED = true
        -   JULIA_DEBUG = Literate
        -   JULIA_CPU_THREADS = 2
        -   JULIA_NUM_THREADS = 48
        -   JULIA_LOAD_PATH = @:@v#.#:@stdlib
        -   JULIA_CUDA_HARD_MEMORY_LIMIT = 25%
        - 
        - CUDA runtime 12.3, artifact installation
        - CUDA driver 12.3
        - NVIDIA driver 545.23.8
        - 
        - CUDA libraries: 
        - - CUBLAS: 12.3.4
        - - CURAND: 10.3.4
        - - CUFFT: 11.0.12
        - - CUSOLVER: 11.5.4
        - - CUSPARSE: 12.2.0
        - - CUPTI: 21.0.0
        - - NVML: 12.0.0+545.23.8
        - 
        - Julia packages: 
        - - CUDA: 5.2.0
        - - CUDA_Driver_jll: 0.7.0+1
        - - CUDA_Runtime_jll: 0.11.1+0
        - 
        - Toolchain:
        - - Julia: 1.10.2
        - - LLVM: 15.0.7
        - 
        - Environment:
        - - JULIA_CUDA_HARD_MEMORY_LIMIT: 25%
        - 
        - 1 device:
        -   0: NVIDIA A100-PCIE-40GB MIG 1g.5gb (sm_80, 4.224 GiB / 4.750 GiB available)
        - ┌ Warning: LuxAMDGPU is loaded but the AMDGPU is not functional.
        - └ @ LuxAMDGPU ~/.cache/julia-buildkite-plugin/depots/01872db4-8c79-43af-ab7d-12abac4f24f6/packages/LuxAMDGPU/sGa0S/src/LuxAMDGPU.jl:19
        - 
        - ````
        - 
        - ---
        - 
        - *This page was generated using [Literate.jl](https://github.com/fredrikekre/Literate.jl).*
        - 
